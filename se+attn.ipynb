{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "from MultiheadAttention import MultiheadAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvInputModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvInputModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(24)\n",
    "        self.conv2 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(24)\n",
    "        self.conv3 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(24)\n",
    "        self.conv4 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(24)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        \"\"\"convolution\"\"\"\n",
    "        x = self.conv1(img)\n",
    "        x = self.batchNorm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchNorm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchNorm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchNorm4(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qembed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEmbedModel(nn.Module):\n",
    "    def __init__(self, in_size, embed=32, hidden=128):\n",
    "        super(QuestionEmbedModel, self).__init__()\n",
    "        \n",
    "        self.wembedding = nn.Embedding(in_size + 1, embed)  #word embeddings have size 32\n",
    "        self.lstm = nn.LSTM(embed, hidden, batch_first=True)  # Input dim is 32, output dim is the question embedding\n",
    "        self.hidden = hidden\n",
    "        \n",
    "    def forward(self, question):\n",
    "        #calculate question embeddings\n",
    "        wembed = self.wembedding(question)\n",
    "        # wembed = wembed.permute(1,0,2) # in lstm minibatches are in the 2-nd dimension\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, hidden = self.lstm(wembed) # initial state is set to zeros by default\n",
    "        qst_emb = hidden[0] # hidden state of the lstm. qst = (B x 128)\n",
    "        #qst_emb = qst_emb.permute(1,0,2).contiguous()\n",
    "        #qst_emb = qst_emb.view(-1, self.hidden*2)\n",
    "        qst_emb = qst_emb[0]\n",
    "        \n",
    "        return qst_emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RLBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalLayerBase(nn.Module):\n",
    "    def __init__(self, in_size, out_size, qst_size, hyp):\n",
    "        super().__init__()\n",
    "\n",
    "        # f_fc1\n",
    "        self.f_fc1 = nn.Linear(hyp[\"g_layers\"][-1], hyp[\"f_fc1\"])\n",
    "        self.mha_fc1 = MultiheadAttention(hyp[\"g_layers\"][-1], MULTIHEADATTENTION_HEADS)\n",
    "        self.identity_fc1 = nn.Identity()\n",
    "        # f_fc2\n",
    "        self.f_fc2 = nn.Linear(hyp[\"f_fc1\"], hyp[\"f_fc2\"])\n",
    "        self.mha_fc2 = MultiheadAttention(hyp[\"f_fc1\"], MULTIHEADATTENTION_HEADS)\n",
    "        self.identity_fc2 = nn.Identity()\n",
    "        # f_fc3\n",
    "        self.f_fc3 = nn.Linear(hyp[\"f_fc2\"], out_size)\n",
    "        self.mha_fc3 = MultiheadAttention(hyp[\"f_fc2\"], MULTIHEADATTENTION_HEADS)\n",
    "        self.identity_fc3 = nn.Identity()\n",
    "    \n",
    "        self.dropout = nn.Dropout(p=hyp[\"dropout\"])\n",
    "        \n",
    "        self.on_gpu = False\n",
    "        self.hyp = hyp\n",
    "        self.qst_size = qst_size\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        self.on_gpu = True\n",
    "        super().cuda(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelationalLayer(RelationalLayerBase):\n",
    "    def __init__(self, in_size, out_size, qst_size, hyp, extraction=False):\n",
    "        super().__init__(in_size, out_size, qst_size, hyp)\n",
    "\n",
    "        self.quest_inject_position = hyp[\"question_injection_position\"]\n",
    "        self.in_size = in_size\n",
    "\n",
    "\t    #create all g layers\n",
    "        self.g_layers = []\n",
    "        self.g_layers_size = hyp[\"g_layers\"]\n",
    "\n",
    "        #create all multiheadattention layers\n",
    "        self.mha_layers = []\n",
    "        self.identity_layers = []\n",
    "\n",
    "        for idx,g_layer_size in enumerate(hyp[\"g_layers\"]):\n",
    "            in_s = in_size if idx==0 else hyp[\"g_layers\"][idx-1]\n",
    "            out_s = g_layer_size\n",
    "            if idx==self.quest_inject_position:\n",
    "                #create the h layer. Now, for better code organization, it is part of the g layers pool. \n",
    "                l = nn.Linear(in_s+qst_size, out_s)\n",
    "                mha = MultiheadAttention(in_s+qst_size, MULTIHEADATTENTION_HEADS)\n",
    "            else:\n",
    "                #create a standard g layer.\n",
    "                l = nn.Linear(in_s, out_s)\n",
    "                mha = MultiheadAttention(in_s, MULTIHEADATTENTION_HEADS)\n",
    "            self.g_layers.append(l)\n",
    "            self.mha_layers.append(mha)\n",
    "            self.identity_layers.append(nn.Identity())\n",
    "\n",
    "\n",
    "        self.g_layers = nn.ModuleList(self.g_layers)\n",
    "        self.mha_layers = nn.ModuleList(self.mha_layers)\n",
    "        self.identity_layers = nn.ModuleList(self.identity_layers)\n",
    "        self.extraction = extraction\n",
    "    \n",
    "    def forward(self, x, qst):\n",
    "        # x = (B x 8*8 x 24)\n",
    "        # qst = (B x 128)\n",
    "        \"\"\"g\"\"\"\n",
    "        b, d, k = x.size()\n",
    "        qst_size = qst.size()[1]\n",
    "        l1_reg = 0\n",
    "        \n",
    "        # add question everywhere\n",
    "        qst = torch.unsqueeze(qst, 1)                      # (B x 1 x 128)\n",
    "        query = qst.clone().transpose(1, 0)\n",
    "        qst = qst.repeat(1, d, 1)                       # (B x 64 x 128)\n",
    "        qst = torch.unsqueeze(qst, 2)                      # (B x 64 x 1 x 128)\n",
    "        \n",
    "        # cast all pairs against each other\n",
    "        x_i = torch.unsqueeze(x, 1)                   # (B x 1 x 64 x 26)\n",
    "        x_i = x_i.repeat(1, d, 1, 1)                    # (B x 64 x 64 x 26)\n",
    "        x_j = torch.unsqueeze(x, 2)                   # (B x 64 x 1 x 26)\n",
    "        #x_j = torch.cat([x_j, qst], 3)\n",
    "        x_j = x_j.repeat(1, 1, d, 1)                    # (B x 64 x 64 x 26)\n",
    "        \n",
    "        # concatenate all together\n",
    "        x_full = torch.cat([x_i, x_j], 3)                  # (B x 64 x 64 x 2*26)\n",
    "        \n",
    "        # reshape for passing through network\n",
    "        x_ = x_full.view(b * d**2, self.in_size)\n",
    "\n",
    "        #create g and inject the question at the position pointed by quest_inject_position.\n",
    "        for idx, (g_layer, mha_layer, g_layer_size, identity) in enumerate(zip(self.g_layers, self.mha_layers, self.g_layers_size, self.identity_layers)):\n",
    "            if idx==self.quest_inject_position:\n",
    "                in_size = self.in_size if idx==0 else self.g_layers_size[idx-1]\n",
    "\n",
    "                # questions inserted\n",
    "                x_img = x_.view(b,d,d,in_size)\n",
    "                qst = qst.repeat(1,1,d,1)\n",
    "                x_concat = torch.cat([x_img,qst],3) #(B x 64 x 64 x 128 + 2 * 26)\n",
    "\n",
    "                # h layer\n",
    "                x_ = x_concat.view(b*(d**2),in_size+self.qst_size)\n",
    "                x_ = g_layer(x_)\n",
    "                x_ = F.relu(x_)\n",
    "            else:\n",
    "                x_ = g_layer(x_)\n",
    "                x_ = F.relu(x_)\n",
    "                # Pass through multiheadattention layer\n",
    "                weights = torch.unsqueeze(g_layer.weight, 0).repeat(b, 1, 1).transpose(1, 0)\n",
    "                _, attn_output_weights = mha_layer(query, weights, weights)\n",
    "                l1_reg += (attn_output_weights.abs().sum() / (attn_output_weights.size(0) * attn_output_weights.size(2)))\n",
    "                attn_output_weights = attn_output_weights.repeat(1, d**2, 1)\n",
    "                # Apply attn_output_weights to x_\n",
    "                x_ = x_.view(b, d**2, g_layer_size) * attn_output_weights\n",
    "                x_ = x_.view(b * (d ** 2), g_layer_size)\n",
    "            x_ = identity(x_)\n",
    "\n",
    "        if self.extraction:\n",
    "            return None\n",
    "        \n",
    "        # reshape again and sum\n",
    "        x_g = x_.view(b, d**2, self.g_layers_size[-1])\n",
    "        x_g = x_g.sum(1).squeeze(1)\n",
    "        \n",
    "        \"\"\"f\"\"\"\n",
    "        # f_fc1\n",
    "        x_f = self.f_fc1(x_g)\n",
    "        x_f = F.relu(x_f)\n",
    "        weights = torch.unsqueeze(self.f_fc1.weight, 0).repeat(b, 1, 1).transpose(1, 0)\n",
    "        _, attn_output_weights = self.mha_fc1(query, weights, weights)\n",
    "        l1_reg += (attn_output_weights.abs().sum() / (attn_output_weights.size(0) * attn_output_weights.size(2)))\n",
    "        x_f = x_f * attn_output_weights.squeeze(1)\n",
    "        x_f = self.identity_fc1(x_f)\n",
    "        # f_fc2\n",
    "        x_f = self.f_fc2(x_f)\n",
    "        x_f = self.dropout(x_f)\n",
    "        x_f = F.relu(x_f)\n",
    "        weights = torch.unsqueeze(self.f_fc2.weight, 0).repeat(b, 1, 1).transpose(1, 0)\n",
    "        _, attn_output_weights = self.mha_fc2(query, weights, weights)\n",
    "        l1_reg += (attn_output_weights.abs().sum() / (attn_output_weights.size(0) * attn_output_weights.size(2)))\n",
    "        x_f = x_f * attn_output_weights.squeeze(1)\n",
    "        x_f = self.identity_fc2(x_f)\n",
    "        # f_fc3\n",
    "        x_f = self.f_fc3(x_f)\n",
    "        weights = torch.unsqueeze(self.f_fc3.weight, 0).repeat(b, 1, 1).transpose(1, 0)\n",
    "        _, attn_output_weights = self.mha_fc3(query, weights, weights)\n",
    "        l1_reg += (attn_output_weights.abs().sum() / (attn_output_weights.size(0) * attn_output_weights.size(2)))\n",
    "        x_f = x_f * attn_output_weights.squeeze(1)\n",
    "        x_f = self.identity_fc3(x_f)\n",
    "        return F.log_softmax(x_f, dim=1), l1_reg "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RN(nn.Module):\n",
    "    def __init__(self, args, hyp, extraction=False):\n",
    "        super(RN, self).__init__()\n",
    "        self.coord_tensor = None\n",
    "        self.on_gpu = False\n",
    "        \n",
    "        # CNN\n",
    "        self.conv = ConvInputModel()\n",
    "        self.state_desc = hyp['state_description']            \n",
    "            \n",
    "        # LSTM\n",
    "        hidden_size = hyp[\"lstm_hidden\"]\n",
    "        self.text = QuestionEmbedModel(args.qdict_size, embed=hyp[\"lstm_word_emb\"], hidden=hidden_size)\n",
    "        \n",
    "        # RELATIONAL LAYER\n",
    "        self.rl_in_size = hyp[\"rl_in_size\"]\n",
    "        self.rl_out_size = args.adict_size\n",
    "        self.rl = RelationalLayer(self.rl_in_size, self.rl_out_size, hidden_size, hyp, extraction) \n",
    "        if hyp[\"question_injection_position\"] != 0:          \n",
    "            print('Supposing IR model')\n",
    "        else:     \n",
    "            print('Supposing original DeepMind model')\n",
    "\n",
    "    def forward(self, img, qst_idxs):\n",
    "        if self.state_desc:\n",
    "            x = img # (B x 12 x 8)\n",
    "        else:\n",
    "            x = self.conv(img)  # (B x 24 x 8 x 8)\n",
    "            b, k, d, _ = x.size()\n",
    "            x = x.view(b,k,d*d) # (B x 24 x 8*8)\n",
    "            \n",
    "            # add coordinates\n",
    "            if self.coord_tensor is None or torch.cuda.device_count() == 1:\n",
    "                self.build_coord_tensor(b, d)                  # (B x 2 x 8 x 8)\n",
    "                self.coord_tensor = self.coord_tensor.view(b,2,d*d) # (B x 2 x 8*8)\n",
    "            \n",
    "            x = torch.cat([x, self.coord_tensor], 1)    # (B x 24+2 x 8*8)\n",
    "            x = x.permute(0, 2, 1)    # (B x 64 x 24+2)\n",
    "        \n",
    "        qst = self.text(qst_idxs)\n",
    "        y = self.rl(x, qst)\n",
    "        return y\n",
    "       \n",
    "    # prepare coord tensor\n",
    "    def build_coord_tensor(self, b, d):\n",
    "        coords = torch.linspace(-d/2., d/2., d)\n",
    "        x = coords.unsqueeze(0).repeat(d, 1)\n",
    "        y = coords.unsqueeze(1).repeat(1, d)\n",
    "        ct = torch.stack((x,y))\n",
    "        # broadcast to all batches\n",
    "        # TODO: upgrade pytorch and use broadcasting\n",
    "        ct = ct.unsqueeze(0).repeat(b, 1, 1, 1)\n",
    "        self.coord_tensor = Variable(ct, requires_grad=False)\n",
    "        if self.on_gpu:\n",
    "            self.coord_tensor = self.coord_tensor.cuda()\n",
    "    \n",
    "    def cuda(self, device=None):\n",
    "        self.on_gpu = True\n",
    "        self.rl.cuda(device)\n",
    "        super(RN, self).cuda(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SE+ATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTIHEADATTENTION_HEADS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(weights):\n",
    "    return weights.mean(dim=1)\n",
    "\n",
    "class SEAttend(nn.Module):\n",
    "    def __init__(self, in_dim=256, out_dim=256, squeeze_dim=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = in_dim # Cantidad de neuronas capa anterior\n",
    "        self.out_dim = out_dim # Cantidad de neuronas capa siguiente (mascara sobre estas)\n",
    "        \n",
    "        self.excite = nn.Sequential(\n",
    "            nn.Linear(out_dim, squeeze_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(squeeze_dim, out_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.attend = MultiheadAttention(\n",
    "            in_dim,\n",
    "            MULTIHEADATTENTION_HEADS,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "    \n",
    "    def forward(self, qst, weights):\n",
    "        bsz = qst.size(0)\n",
    "        \n",
    "        scale = self.squeeze(weights) # scale: [out_dim (256)]\n",
    "        scale = self.excite(scale.unsqueeze(0)) # scale: [1, out_dim (256)]\n",
    "        weights = weights * scale.t() # weights: [out_dim, in_dim]\n",
    "        weights = weights.unsqueeze(1).expand(self.out_dim, bsz, self.in_dim) # weights: [out_dim, bsz, in_dim]\n",
    "        \n",
    "        _, attn_output_weights = self.attend(qst.unsqueeze(0), weights, weights)\n",
    "        \n",
    "        # Retorno None para mantener el formato\n",
    "        return None, attn_output_weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def squeeze(weights):\n",
    "        return squeeze(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 256])\n",
      "tensor([[[0.3954, 0.0000, 0.3832, 0.7434, 0.9056, 0.7858, 0.3215, 0.0000,\n",
      "          0.6960, 0.3804, 0.4661, 0.3918, 0.8876, 0.9478, 0.7458, 0.7132,\n",
      "          0.6596, 0.8342, 0.2089, 0.4842, 0.0000, 0.8956, 0.5129, 0.3723,\n",
      "          0.4412, 0.9768, 0.4684, 0.1167, 0.0000, 0.4218, 0.2370, 0.2961,\n",
      "          0.6493, 0.5177, 0.2612, 0.0000, 0.4412, 0.6539, 0.1700, 0.7018,\n",
      "          0.7742, 0.6343, 0.3036, 0.6198, 0.0000, 0.6323, 0.8880, 0.3315,\n",
      "          0.4574, 0.3043, 0.2708, 0.5403, 0.4379, 0.5808, 0.0964, 0.6952,\n",
      "          0.4946, 0.8681, 0.2941, 0.3637, 0.3320, 0.1906, 0.2941, 0.1529,\n",
      "          0.6513, 0.6320, 0.9429, 0.8656, 0.7018, 0.9351, 0.1732, 0.7580,\n",
      "          0.0000, 0.7699, 0.7505, 0.6371, 0.7836, 0.4200, 0.7134, 0.2379,\n",
      "          0.5477, 0.7258, 0.3581, 0.0000, 0.4896, 0.6800, 0.8775, 0.4228,\n",
      "          0.2313, 0.7232, 0.9392, 0.6202, 0.5477, 0.6598, 0.4646, 0.9394,\n",
      "          0.0000, 0.5675, 0.3592, 0.0000, 0.6514, 0.5870, 0.6984, 0.5621,\n",
      "          0.4417, 0.6087, 0.6726, 0.3114, 0.6205, 1.0339, 0.4443, 0.1919,\n",
      "          0.2673, 0.0000, 0.6569, 0.4951, 0.1424, 0.4796, 0.3348, 0.8303,\n",
      "          0.8947, 0.7682, 0.6163, 0.6683, 0.2712, 0.6071, 0.5313, 0.7467,\n",
      "          0.4713, 0.2946, 0.5233, 0.3194, 0.8072, 0.1361, 0.4813, 0.7575,\n",
      "          0.0000, 0.4805, 0.3521, 0.6693, 0.4119, 0.4373, 0.1600, 0.8509,\n",
      "          0.5203, 0.7085, 0.4298, 0.7100, 0.7368, 0.8847, 0.6107, 0.9655,\n",
      "          0.7832, 0.6788, 0.5570, 0.6764, 0.7794, 0.8347, 0.2238, 0.4318,\n",
      "          0.7774, 0.7936, 0.4841, 0.7284, 0.7067, 0.4151, 0.6592, 0.6573,\n",
      "          0.0000, 0.5104, 0.4762, 0.5415, 0.4891, 0.8085, 0.0000, 0.1812,\n",
      "          0.6106, 0.7940, 0.1294, 0.6090, 0.0000, 0.8970, 0.6433, 0.3590,\n",
      "          0.3140, 0.5729, 0.2247, 0.2250, 0.8488, 0.7841, 0.2530, 0.0000,\n",
      "          0.7529, 0.3575, 0.1605, 0.5180, 0.4126, 0.5288, 0.0000, 0.8440,\n",
      "          0.1476, 0.5675, 0.4750, 0.9537, 0.3232, 0.7637, 0.7654, 0.5041,\n",
      "          0.2230, 0.9641, 0.5342, 1.0520, 0.8220, 0.4773, 0.3529, 0.6837,\n",
      "          0.0000, 0.6473, 1.0462, 0.3137, 0.3191, 0.5689, 0.8645, 0.0000,\n",
      "          0.1227, 0.6847, 0.0000, 0.5749, 0.1819, 0.4587, 0.3868, 0.2819,\n",
      "          0.0000, 0.5821, 0.0000, 0.7016, 0.0000, 0.6222, 0.4856, 0.2495,\n",
      "          0.9680, 0.2284, 0.7682, 0.0000, 0.3630, 0.8745, 0.4184, 0.2943,\n",
      "          0.0000, 0.0000, 0.6562, 0.2603, 0.3165, 0.0000, 0.6932, 0.5505]],\n",
      "\n",
      "        [[0.3758, 0.8335, 0.0952, 0.6078, 0.6208, 0.3173, 0.5678, 0.5014,\n",
      "          0.9626, 0.0000, 0.0000, 0.4244, 0.7948, 0.1275, 0.8185, 0.1337,\n",
      "          0.4852, 0.5884, 0.2676, 0.4147, 0.9795, 0.2253, 0.7501, 0.6046,\n",
      "          0.7009, 0.3406, 0.6774, 0.5120, 1.0224, 0.6303, 0.8762, 0.8642,\n",
      "          0.7320, 0.5824, 0.4734, 0.7090, 0.7569, 0.5279, 0.4711, 0.5744,\n",
      "          0.0000, 0.4150, 0.7512, 0.3356, 0.2361, 0.0000, 0.3719, 0.0000,\n",
      "          0.6262, 0.4902, 0.3630, 0.0000, 0.7961, 0.6474, 0.9169, 0.9766,\n",
      "          0.5276, 0.6690, 0.8629, 0.0000, 0.2179, 0.8348, 0.7568, 0.8104,\n",
      "          0.0000, 0.4212, 0.6478, 0.1491, 0.5699, 0.0000, 0.6249, 0.8022,\n",
      "          0.6289, 0.0000, 0.4901, 0.4858, 0.0000, 0.4686, 0.3790, 0.9738,\n",
      "          0.5394, 0.1201, 0.6375, 0.2381, 0.6229, 0.6238, 0.2310, 0.0000,\n",
      "          0.6353, 0.0000, 0.5628, 0.6253, 0.2578, 0.1564, 0.2868, 0.3201,\n",
      "          0.7504, 0.1865, 0.0000, 0.5062, 0.3485, 0.9053, 0.6639, 0.5072,\n",
      "          0.6324, 0.0000, 0.5793, 0.7918, 0.2808, 0.7035, 0.7084, 0.8986,\n",
      "          0.6850, 0.4099, 0.4768, 0.0000, 0.9076, 0.8118, 0.1535, 0.6224,\n",
      "          0.8690, 0.6052, 0.3422, 0.1995, 0.6334, 0.4370, 0.3504, 0.2787,\n",
      "          0.5217, 0.5392, 0.7338, 0.7440, 0.6010, 0.9252, 0.8642, 0.7769,\n",
      "          0.9416, 0.4862, 0.5626, 0.2962, 0.5561, 0.6782, 0.3646, 0.0000,\n",
      "          0.2033, 0.6936, 0.6501, 0.5967, 0.6820, 0.5981, 0.5003, 0.5315,\n",
      "          0.9881, 0.7527, 0.5516, 0.0000, 0.3353, 0.5513, 0.5275, 0.8624,\n",
      "          0.5324, 0.5905, 0.5077, 0.7885, 0.0000, 0.0000, 0.2079, 0.2096,\n",
      "          0.0000, 0.6046, 0.6795, 0.1915, 0.3830, 0.6098, 0.6422, 0.2914,\n",
      "          0.5810, 0.7145, 0.7171, 0.3037, 0.9356, 0.4679, 0.2636, 0.4822,\n",
      "          0.8887, 0.3783, 0.5090, 0.4317, 0.0000, 0.0000, 0.8421, 0.4487,\n",
      "          0.7551, 0.7388, 0.4626, 0.8574, 0.4093, 0.8087, 0.1355, 0.5103,\n",
      "          0.9719, 0.9503, 0.3810, 0.2961, 0.4736, 0.2862, 0.5784, 0.5051,\n",
      "          0.6209, 0.7785, 0.5777, 0.4555, 0.5632, 0.3519, 0.2564, 0.9228,\n",
      "          0.5116, 0.4300, 0.2139, 0.7832, 0.4261, 1.0174, 0.1776, 0.3088,\n",
      "          0.0000, 0.6629, 0.3752, 0.9058, 0.6671, 0.6386, 0.3036, 0.6359,\n",
      "          0.2733, 0.6373, 0.3605, 0.3161, 0.3261, 0.3012, 0.4394, 0.2681,\n",
      "          0.1393, 0.7539, 0.2806, 0.3318, 0.6560, 0.7437, 0.2215, 0.5890,\n",
      "          0.6725, 1.0166, 0.7587, 0.9162, 0.6036, 0.3760, 0.3422, 0.8627]]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(255, 256)\n",
    "sea = SEAttend(out_dim=256, in_dim=255)\n",
    "qst = torch.randn(2, 255)\n",
    "_, ret = sea(qst, layer.weight)\n",
    "print(ret.size())\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 255])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cada fila representa todos los pesos que llegan a una\n",
    "# neurona de la capa siguiente\n",
    "layer.weight.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = layer.weight.mean(dim=1)\n",
    "means.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.randn()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
