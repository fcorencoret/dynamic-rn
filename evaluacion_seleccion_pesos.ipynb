{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from MultiheadAttention import MultiheadAttention\n",
    "\n",
    "\n",
    "MULTIHEADATTENTION_HEADS = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, r = 16):\n",
    "        super(SE, self).__init__()\n",
    "\n",
    "        self.r = r\n",
    "        self.scale = int(input_dim/self.r)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, self.scale)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(self.scale, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.norm = nn.InstanceNorm1d(1, affine = False)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tgt_len, bsz, embed_dim = x.size()\n",
    "        out = self.relu(self.fc1(x.view(bsz, embed_dim)))\n",
    "        out = self.fc2(out)\n",
    "        out = self.norm(out.unsqueeze(1))\n",
    "        out = self.sigmoid(out)     \n",
    "\n",
    "        return out, out.view(bsz, 1, self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squeeze(weights):\n",
    "    return weights.mean(dim=1)\n",
    "\n",
    "class SEAttend(nn.Module):\n",
    "    def __init__(self, in_dim=256, out_dim=256, squeeze_dim=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = in_dim # Cantidad de neuronas capa anterior\n",
    "        self.out_dim = out_dim # Cantidad de neuronas capa siguiente (mascara sobre estas)\n",
    "        \n",
    "        self.excite = nn.Sequential(\n",
    "            nn.Linear(out_dim, squeeze_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(squeeze_dim, out_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.attend = MultiheadAttention(\n",
    "            in_dim,\n",
    "            MULTIHEADATTENTION_HEADS,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "    \n",
    "    def forward(self, qst, weights):\n",
    "        bsz = qst.size(0)\n",
    "        \n",
    "        scale = self.squeeze(weights) # scale: [out_dim (256)]\n",
    "        scale = self.excite(scale.unsqueeze(0)) # scale: [1, out_dim (256)]\n",
    "        weights = weights * scale.t() # weights: [out_dim, in_dim]\n",
    "        weights = weights.unsqueeze(1).expand(self.out_dim, bsz, self.in_dim) # weights: [out_dim, bsz, in_dim]\n",
    "        \n",
    "        _, attn_output_weights = self.attend(qst, weights, weights)\n",
    "        \n",
    "        # Retorno None para mantener el formato\n",
    "        return None, attn_output_weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def squeeze(weights):\n",
    "        return squeeze(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvInputModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvInputModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(24)\n",
    "        self.conv2 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(24)\n",
    "        self.conv3 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(24)\n",
    "        self.conv4 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(24)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        \"\"\"convolution\"\"\"\n",
    "        x = self.conv1(img)\n",
    "        x = self.batchNorm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchNorm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchNorm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchNorm4(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QuestionEmbedModel(nn.Module):\n",
    "    def __init__(self, in_size, embed=32, hidden=128):\n",
    "        super(QuestionEmbedModel, self).__init__()\n",
    "        \n",
    "        self.wembedding = nn.Embedding(in_size + 1, embed)  #word embeddings have size 32\n",
    "        self.lstm = nn.LSTM(embed, hidden, batch_first=True)  # Input dim is 32, output dim is the question embedding\n",
    "        self.hidden = hidden\n",
    "        \n",
    "    def forward(self, question):\n",
    "        #calculate question embeddings\n",
    "        wembed = self.wembedding(question)\n",
    "        # wembed = wembed.permute(1,0,2) # in lstm minibatches are in the 2-nd dimension\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, hidden = self.lstm(wembed) # initial state is set to zeros by default\n",
    "        qst_emb = hidden[0] # hidden state of the lstm. qst = (B x 128)\n",
    "        #qst_emb = qst_emb.permute(1,0,2).contiguous()\n",
    "        #qst_emb = qst_emb.view(-1, self.hidden*2)\n",
    "        qst_emb = qst_emb[0]\n",
    "        \n",
    "        return qst_emb\n",
    "\n",
    "class RelationalLayerBase(nn.Module):\n",
    "    def __init__(self, in_size, out_size, qst_size, hyp):\n",
    "        super().__init__()\n",
    "\n",
    "        # f_fc1\n",
    "        self.f_fc1 = nn.Linear(hyp[\"g_layers\"][-1], hyp[\"f_fc1\"])\n",
    "        self.mha_fc1 = SEAttend(hyp[\"g_layers\"][-1], hyp[\"f_fc1\"])\n",
    "        self.identity_fc1 = nn.Identity()\n",
    "        # f_fc2\n",
    "        self.f_fc2 = nn.Linear(hyp[\"f_fc1\"], hyp[\"f_fc2\"])\n",
    "        self.mha_fc2 = SEAttend(hyp[\"f_fc1\"], hyp[\"f_fc2\"])\n",
    "        self.identity_fc2 = nn.Identity()\n",
    "        # f_fc3\n",
    "        self.f_fc3 = nn.Linear(hyp[\"f_fc2\"], out_size)\n",
    "        self.mha_fc3 = SEAttend(hyp[\"f_fc2\"], out_size)\n",
    "        self.identity_fc3 = nn.Identity()\n",
    "    \n",
    "        self.dropout = nn.Dropout(p=hyp[\"dropout\"])\n",
    "        \n",
    "        self.on_gpu = False\n",
    "        self.hyp = hyp\n",
    "        self.qst_size = qst_size\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        self.on_gpu = True\n",
    "        super().cuda(device)\n",
    "    \n",
    "\n",
    "class RelationalLayer(RelationalLayerBase):\n",
    "    def __init__(self, in_size, out_size, qst_size, hyp, extraction=False):\n",
    "        super().__init__(in_size, out_size, qst_size, hyp)\n",
    "\n",
    "        self.quest_inject_position = hyp[\"question_injection_position\"]\n",
    "        self.in_size = in_size\n",
    "\n",
    "\t    #create all g layers\n",
    "        self.g_layers = []\n",
    "        self.g_layers_size = hyp[\"g_layers\"]\n",
    "\n",
    "        #create all multiheadattention layers\n",
    "        self.mha_layers = []\n",
    "        self.identity_layers = []\n",
    "\n",
    "        for idx,g_layer_size in enumerate(hyp[\"g_layers\"]):\n",
    "            in_s = in_size if idx==0 else hyp[\"g_layers\"][idx-1]\n",
    "            out_s = g_layer_size\n",
    "            if idx==self.quest_inject_position:\n",
    "                #create the h layer. Now, for better code organization, it is part of the g layers pool. \n",
    "                l = nn.Linear(in_s+qst_size, out_s)\n",
    "                mha = SEAttend(in_s+qst_size, out_s)\n",
    "            else:\n",
    "                #create a standard g layer.\n",
    "                l = nn.Linear(in_s, out_s)\n",
    "                mha = SEAttend(in_s, out_s)\n",
    "            self.g_layers.append(l)\n",
    "            self.mha_layers.append(mha)\n",
    "            self.identity_layers.append(nn.Identity())\n",
    "\n",
    "\n",
    "        self.g_layers = nn.ModuleList(self.g_layers)\n",
    "        self.mha_layers = nn.ModuleList(self.mha_layers)\n",
    "        self.identity_layers = nn.ModuleList(self.identity_layers)\n",
    "        self.extraction = extraction\n",
    "    \n",
    "    def forward(self, x, qst):\n",
    "        # x = (B x 8*8 x 24)\n",
    "        # qst = (B x 128)\n",
    "        \"\"\"g\"\"\"\n",
    "        b, d, k = x.size()\n",
    "        qst_size = qst.size()[1]\n",
    "        l1_reg = 0\n",
    "        \n",
    "        # add question everywhere\n",
    "        qst = torch.unsqueeze(qst, 1)                      # (B x 1 x 128)\n",
    "        query = qst.clone().transpose(1, 0)\n",
    "        qst = qst.repeat(1, d, 1)                       # (B x 64 x 128)\n",
    "        qst = torch.unsqueeze(qst, 2)                      # (B x 64 x 1 x 128)\n",
    "        \n",
    "        # cast all pairs against each other\n",
    "        x_i = torch.unsqueeze(x, 1)                   # (B x 1 x 64 x 26)\n",
    "        x_i = x_i.repeat(1, d, 1, 1)                    # (B x 64 x 64 x 26)\n",
    "        x_j = torch.unsqueeze(x, 2)                   # (B x 64 x 1 x 26)\n",
    "        #x_j = torch.cat([x_j, qst], 3)\n",
    "        x_j = x_j.repeat(1, 1, d, 1)                    # (B x 64 x 64 x 26)\n",
    "        \n",
    "        # concatenate all together\n",
    "        x_full = torch.cat([x_i, x_j], 3)                  # (B x 64 x 64 x 2*26)\n",
    "        \n",
    "        # reshape for passing through network\n",
    "        x_ = x_full.view(b * d**2, self.in_size)\n",
    "\n",
    "        #create g and inject the question at the position pointed by quest_inject_position.\n",
    "        for idx, (g_layer, mha_layer, g_layer_size, identity) in enumerate(zip(self.g_layers, self.mha_layers, self.g_layers_size, self.identity_layers)):\n",
    "            if idx==self.quest_inject_position:\n",
    "                in_size = self.in_size if idx==0 else self.g_layers_size[idx-1]\n",
    "\n",
    "                # questions inserted\n",
    "                x_img = x_.view(b,d,d,in_size)\n",
    "                qst = qst.repeat(1,1,d,1)\n",
    "                x_concat = torch.cat([x_img,qst],3) #(B x 64 x 64 x 128 + 2 * 26)\n",
    "\n",
    "                # h layer\n",
    "                x_ = x_concat.view(b*(d**2),in_size+self.qst_size)\n",
    "                x_ = g_layer(x_)\n",
    "                x_ = F.relu(x_)\n",
    "            else:\n",
    "                x_ = g_layer(x_)\n",
    "                x_ = F.relu(x_)\n",
    "                # Pass through multiheadattention layer\n",
    "                weights = g_layer.weight\n",
    "                # weights = torch.unsqueeze(g_layer.weight, 0).repeat(b, 1, 1).transpose(1, 0)\n",
    "                print('wights', weights.size())\n",
    "                print(query.size())\n",
    "                _, attn_output_weights = mha_layer(query, weights)\n",
    "                l1_reg += (attn_output_weights.abs().sum() / (attn_output_weights.size(0) * attn_output_weights.size(2)))\n",
    "                \n",
    "                print('attn size', attn_output_weights.size())\n",
    "\n",
    "                attn_output_weights = attn_output_weights.expand(b, d**2, attn_output_weights.size(-1))\n",
    "                \n",
    "                # Apply attn_output_weights to x_\n",
    "                print('attn size', attn_output_weights.size())\n",
    "                print('x size', x_.view(b, d**2, g_layer_size).size())\n",
    "                x_ = x_.view(b, d**2, g_layer_size) * attn_output_weights\n",
    "                x_ = x_.view(b * (d ** 2), g_layer_size)\n",
    "            x_ = identity(x_)\n",
    "\n",
    "        if self.extraction:\n",
    "            return None\n",
    "        \n",
    "        # reshape again and sum\n",
    "        x_g = x_.view(b, d**2, self.g_layers_size[-1])\n",
    "        x_g = x_g.sum(1).squeeze(1)\n",
    "        \n",
    "        \"\"\"f\"\"\"\n",
    "        # f_fc1\n",
    "        x_f = self.f_fc1(x_g)\n",
    "        x_f = F.relu(x_f)\n",
    "        weights = self.f_fc1.weight\n",
    "        # weights = torch.unsqueeze(self.f_fc1.weight, 0).repeat(b, 1, 1).transpose(1, 0)\n",
    "        _, attn_output_weights = self.mha_fc1(query, weights)\n",
    "        l1_reg += (attn_output_weights.abs().sum() / (attn_output_weights.size(0) * attn_output_weights.size(2)))\n",
    "        x_f = x_f * attn_output_weights.squeeze(1)\n",
    "        x_f = self.identity_fc1(x_f)\n",
    "        # f_fc2\n",
    "        x_f = self.f_fc2(x_f)\n",
    "        x_f = self.dropout(x_f)\n",
    "        x_f = F.relu(x_f)\n",
    "        weights = self.f_fc2.weight\n",
    "        # weights = torch.unsqueeze(self.f_fc2.weight, 0).repeat(b, 1, 1).transpose(1, 0)\n",
    "        _, attn_output_weights = self.mha_fc2(query, weights)\n",
    "        l1_reg += (attn_output_weights.abs().sum() / (attn_output_weights.size(0) * attn_output_weights.size(2)))\n",
    "        x_f = x_f * attn_output_weights.squeeze(1)\n",
    "        x_f = self.identity_fc2(x_f)\n",
    "        # f_fc3\n",
    "        x_f = self.f_fc3(x_f)\n",
    "        weights = self.f_fc3.weight \n",
    "        # weights = torch.unsqueeze(self.f_fc3.weight, 0).repeat(b, 1, 1).transpose(1, 0)\n",
    "        _, attn_output_weights = self.mha_fc3(query, weights)\n",
    "        l1_reg += (attn_output_weights.abs().sum() / (attn_output_weights.size(0) * attn_output_weights.size(2)))\n",
    "        x_f = x_f * attn_output_weights.squeeze(1)\n",
    "        x_f = self.identity_fc3(x_f)\n",
    "        return F.log_softmax(x_f, dim=1), l1_reg \n",
    "\n",
    "class RN(nn.Module):\n",
    "    def __init__(self, args, hyp, extraction=False):\n",
    "        super(RN, self).__init__()\n",
    "        self.coord_tensor = None\n",
    "        self.on_gpu = False\n",
    "        \n",
    "        # CNN\n",
    "        self.conv = ConvInputModel()\n",
    "        self.state_desc = hyp['state_description']            \n",
    "            \n",
    "        # LSTM\n",
    "        hidden_size = hyp[\"lstm_hidden\"]\n",
    "        self.text = QuestionEmbedModel(args.qdict_size, embed=hyp[\"lstm_word_emb\"], hidden=hidden_size)\n",
    "        \n",
    "        # RELATIONAL LAYER\n",
    "        self.rl_in_size = hyp[\"rl_in_size\"]\n",
    "        self.rl_out_size = args.adict_size\n",
    "        self.rl = RelationalLayer(self.rl_in_size, self.rl_out_size, hidden_size, hyp, extraction) \n",
    "        if hyp[\"question_injection_position\"] != 0:          \n",
    "            print('Supposing IR model')\n",
    "        else:     \n",
    "            print('Supposing original DeepMind model')\n",
    "\n",
    "    def forward(self, img, qst_idxs):\n",
    "        if self.state_desc:\n",
    "            x = img # (B x 12 x 8)\n",
    "        else:\n",
    "            x = self.conv(img)  # (B x 24 x 8 x 8)\n",
    "            b, k, d, _ = x.size()\n",
    "            x = x.view(b,k,d*d) # (B x 24 x 8*8)\n",
    "            \n",
    "            # add coordinates\n",
    "            if self.coord_tensor is None or torch.cuda.device_count() == 1:\n",
    "                self.build_coord_tensor(b, d)                  # (B x 2 x 8 x 8)\n",
    "                self.coord_tensor = self.coord_tensor.view(b,2,d*d) # (B x 2 x 8*8)\n",
    "            \n",
    "            x = torch.cat([x, self.coord_tensor], 1)    # (B x 24+2 x 8*8)\n",
    "            x = x.permute(0, 2, 1)    # (B x 64 x 24+2)\n",
    "        \n",
    "        qst = self.text(qst_idxs)\n",
    "        y = self.rl(x, qst)\n",
    "        return y\n",
    "       \n",
    "    # prepare coord tensor\n",
    "    def build_coord_tensor(self, b, d):\n",
    "        coords = torch.linspace(-d/2., d/2., d)\n",
    "        x = coords.unsqueeze(0).repeat(d, 1)\n",
    "        y = coords.unsqueeze(1).repeat(1, d)\n",
    "        ct = torch.stack((x,y))\n",
    "        # broadcast to all batches\n",
    "        # TODO: upgrade pytorch and use broadcasting\n",
    "        ct = ct.unsqueeze(0).repeat(b, 1, 1, 1)\n",
    "        self.coord_tensor = Variable(ct, requires_grad=False)\n",
    "        if self.on_gpu:\n",
    "            self.coord_tensor = self.coord_tensor.cuda()\n",
    "    \n",
    "    def cuda(self, device=None):\n",
    "        self.on_gpu = True\n",
    "        self.rl.cuda(device)\n",
    "        super(RN, self).cuda(device)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dictionaries(clevr_dir):\n",
    "\n",
    "    def compute_class(answer):\n",
    "        for name,values in classes.items():\n",
    "            if answer in values:\n",
    "                return name\n",
    "        \n",
    "        raise ValueError('Answer {} does not belong to a known class'.format(answer))\n",
    "        \n",
    "        \n",
    "    cached_dictionaries = os.path.join('questions', 'CLEVR_built_dictionaries.pkl')\n",
    "    if os.path.exists(cached_dictionaries):\n",
    "        print('==> using cached dictionaries: {}'.format(cached_dictionaries))\n",
    "        with open(cached_dictionaries, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "            \n",
    "    quest_to_ix = {}\n",
    "    answ_to_ix = {}\n",
    "    answ_ix_to_class = {}\n",
    "    json_train_filename = os.path.join(clevr_dir, 'questions', 'CLEVR_train_questions.json')\n",
    "    #load all words from all training data\n",
    "    with open(json_train_filename, \"r\") as f:\n",
    "        questions = json.load(f)['questions']\n",
    "        for q in tqdm(questions):\n",
    "            question = tokenize(q['question'])\n",
    "            answer = q['answer']\n",
    "            #pdb.set_trace()\n",
    "            for word in question:\n",
    "                if word not in quest_to_ix:\n",
    "                    quest_to_ix[word] = len(quest_to_ix)+1 #one based indexing; zero is reserved for padding\n",
    "            \n",
    "            a = answer.lower()\n",
    "            if a not in answ_to_ix:\n",
    "                    ix = len(answ_to_ix)+1\n",
    "                    answ_to_ix[a] = ix\n",
    "                    answ_ix_to_class[ix] = compute_class(a)\n",
    "\n",
    "    ret = (quest_to_ix, answ_to_ix, answ_ix_to_class)    \n",
    "    with open(cached_dictionaries, 'wb') as f:\n",
    "        pickle.dump(ret, f)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data, model, epoch, dictionaries, args):\n",
    "    model.eval()\n",
    "\n",
    "    # accuracy for every class\n",
    "    class_corrects = {}\n",
    "    # for every class, among all the wrong answers, how much are non pertinent\n",
    "    class_invalids = {}\n",
    "    # total number of samples for every class\n",
    "    class_n_samples = {}\n",
    "    # initialization\n",
    "    for c in dictionaries[2].values():\n",
    "        class_corrects[c] = 0.0\n",
    "        class_invalids[c] = 0.0\n",
    "        class_n_samples[c] = 0.0\n",
    "\n",
    "    corrects = 0.0\n",
    "    invalids = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    inverted_answ_dict = {v: k for k,v in dictionaries[1].items()}\n",
    "    sorted_classes = sorted(dictionaries[2].items(), key=lambda x: hash(x[1]) if x[1]!='number' else int(inverted_answ_dict[x[0]]))\n",
    "    sorted_classes = [c[0]-1 for c in sorted_classes]\n",
    "\n",
    "    confusion_matrix_target = []\n",
    "    confusion_matrix_pred = []\n",
    "\n",
    "    sorted_labels = sorted(dictionaries[1].items(), key=lambda x: x[1])\n",
    "    sorted_labels = [c[0] for c in sorted_labels]\n",
    "    sorted_labels = [sorted_labels[c] for c in sorted_classes]\n",
    "\n",
    "    avg_loss = 0.0\n",
    "    progress_bar = tqdm(data)\n",
    "    with torch.set_grad_enabled(True):\n",
    "        for batch_idx, sample_batched in enumerate(progress_bar):\n",
    "            img, qst, label = utils.load_tensor_data(sample_batched, args.cuda, args.invert_questions, volatile=True)\n",
    "\n",
    "            output, l1_reg = model(img, qst)\n",
    "            pred = output.data.max(1)[1]\n",
    "\n",
    "            print(l1_reg)\n",
    "            print(l1_reg.mean())\n",
    "            print('item', l1_reg.mean().item())\n",
    "            loss = F.nll_loss(output, label) + args.l1_lambd * l1_reg.mean()\n",
    "            print(loss)\n",
    "\n",
    "            # compute per-class accuracy\n",
    "            pred_class = [dictionaries[2][o.item()+1] for o in pred]\n",
    "            real_class = [dictionaries[2][o.item()+1] for o in label.data]\n",
    "            for idx,rc in enumerate(real_class):\n",
    "                class_corrects[rc] += (pred[idx] == label.data[idx]).item()\n",
    "                class_n_samples[rc] += 1\n",
    "\n",
    "            for pc, rc in zip(pred_class,real_class):\n",
    "                class_invalids[rc] += (pc != rc)\n",
    "\n",
    "            for p,l in zip(pred, label.data):\n",
    "                confusion_matrix_target.append(sorted_classes.index(l))\n",
    "                confusion_matrix_pred.append(sorted_classes.index(p))\n",
    "\n",
    "            # compute global accuracy\n",
    "            corrects += (pred == label.data).sum().item()\n",
    "            assert corrects == sum(class_corrects.values()), 'Number of correct answers assertion error!'\n",
    "            invalids = sum(class_invalids.values())\n",
    "            n_samples += len(label)\n",
    "            assert n_samples == sum(class_n_samples.values()), 'Number of total answers assertion error!'\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                accuracy = corrects / n_samples\n",
    "                invalids_perc = invalids / n_samples\n",
    "                progress_bar.set_postfix(dict(acc='{:.2%}'.format(accuracy), inv='{:.2%}'.format(invalids_perc)))\n",
    "    \n",
    "    avg_loss /= len(data)\n",
    "    invalids_perc = invalids / n_samples      \n",
    "    global_accuracy = corrects / n_samples\n",
    "\n",
    "    print('Test Epoch {}: Accuracy = {:.2%} ({:g}/{}); Invalids = {:.2%} ({:g}/{}); Test loss = {}'.format(epoch, accuracy, corrects, n_samples, invalids_perc, invalids, n_samples, avg_loss))\n",
    "    for v in class_n_samples.keys():\n",
    "        accuracy = 0\n",
    "        invalid = 0\n",
    "        if class_n_samples[v] != 0:\n",
    "            accuracy = class_corrects[v] / class_n_samples[v]\n",
    "            invalid = class_invalids[v] / class_n_samples[v]\n",
    "        print('{} -- acc: {:.2%} ({}/{}); invalid: {:.2%} ({}/{})'.format(v,accuracy,class_corrects[v],class_n_samples[v],invalid,class_invalids[v],class_n_samples[v]))\n",
    "\n",
    "    dump_object = {\n",
    "        'class_corrects':class_corrects,\n",
    "        'class_invalids':class_invalids,\n",
    "        'class_total_samples':class_n_samples,\n",
    "        'confusion_matrix_target':confusion_matrix_target,\n",
    "        'confusion_matrix_pred':confusion_matrix_pred,\n",
    "        'confusion_matrix_labels':sorted_labels,\n",
    "        'global_accuracy':global_accuracy,\n",
    "        'global_invalids':invalids_perc\n",
    "    }\n",
    "    torch.cuda.empty_cache()\n",
    "    return avg_loss, dump_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from clevr_dataset_connector import ClevrDataset, ClevrDatasetStateDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp =  {\n",
    "    \"state_description\": False,\n",
    "    \"g_layers\": [256,256,256,256],\n",
    "    \"question_injection_position\": 0,\n",
    "\n",
    "    \"f_fc1\": 256,\n",
    "    \"f_fc2\": 256,\n",
    "\n",
    "    \"dropout\": 0.5,\n",
    "    \"lstm_hidden\": 256,\n",
    "    \"lstm_word_emb\": 32,\n",
    "    \"rl_in_size\": 52\n",
    "}\n",
    "\n",
    "args = edict(\n",
    "    batch_size=220,\n",
    "    bs_gamma=1,\n",
    "    bs_max=-1,\n",
    "    bs_step=20,\n",
    "    clevr_dir='/Users/sebamenabar/Documents/datasets/CLEVR/CLEVR_v1.0',\n",
    "    clip_norm=50,\n",
    "    comet=1,\n",
    "    config='config.json',\n",
    "    conv_transfer_learn=None,\n",
    "    dropout=-1, \n",
    "    epochs=400,\n",
    "    experiment='Norm',\n",
    "    freeze_RN=False,\n",
    "    invert_questions=True,\n",
    "    l1_lambd=0.0,\n",
    "    log_interval=10,\n",
    "    lr=5e-06,\n",
    "    lr_gamma=2,\n",
    "    lr_max=0.0005,\n",
    "    lr_step=20,\n",
    "    model='original-fp',\n",
    "    no_cuda=False,\n",
    "    no_invert_questions=False,\n",
    "    question_injection=-1,\n",
    "    resume=None,\n",
    "    resume_comet='',\n",
    "    resume_optimizer=None,\n",
    "    seed=42,\n",
    "    subset=1.0,\n",
    "    test=False,\n",
    "    test_batch_size=100,\n",
    ")\n",
    "\n",
    "if args.dropout > 0:\n",
    "    hyp['dropout'] = args.dropout\n",
    "if args.question_injection >= 0:\n",
    "    hyp['question_injection_position'] = args.question_injection\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> using cached dictionaries: questions/CLEVR_built_dictionaries.pkl\n"
     ]
    }
   ],
   "source": [
    "dictionaries = utils.build_dictionaries(args.clevr_dir, 'clevr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.qdict_size = len(dictionaries[0])\n",
    "args.adict_size = len(dictionaries[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> using cached questions: questions/CLEVR_train_questions.pkl\n",
      "==> using cached questions: questions/CLEVR_val_questions.pkl\n"
     ]
    }
   ],
   "source": [
    "train_transforms = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                   transforms.Pad(8),\n",
    "                                   transforms.RandomCrop((128, 128)),\n",
    "                                   transforms.RandomRotation(2.8),  # .05 rad\n",
    "                                   transforms.ToTensor()])\n",
    "test_transforms = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                  transforms.ToTensor()])\n",
    "\n",
    "clevr_dataset_train = ClevrDataset(args.clevr_dir, True, dictionaries, train_transforms)\n",
    "clevr_dataset_test = ClevrDataset(args.clevr_dir, False, dictionaries, test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_subset = True\n",
    "workers = 4\n",
    "args.bs = 8\n",
    "subset_size = 128\n",
    "\n",
    "clevr_subset_test = Subset(clevr_dataset_test, np.arange(subset_size))\n",
    "\n",
    "if use_subset:\n",
    "    clevr_test_loader = DataLoader(clevr_subset_test, batch_size=args.bs,\n",
    "                                   shuffle=False, num_workers=4, collate_fn=utils.collate_samples_from_pixels)  \n",
    "# clevr_train_loader = DataLoader(clevr_dataset_train, batch_size=args.bs,\n",
    "#                                 shuffle=False, num_workers=4, collate_fn=utils.collate_samples_from_pixels)\n",
    "else:\n",
    "    clevr_test_loader = DataLoader(clevr_dataset_test, batch_size=args.bs,\n",
    "                                   shuffle=False, num_workers=4, collate_fn=utils.collate_samples_from_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RN(\n",
       "  (conv): ConvInputModel(\n",
       "    (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (batchNorm1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv2): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (batchNorm2): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv3): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (batchNorm3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (conv4): Conv2d(24, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (batchNorm4): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (text): QuestionEmbedModel(\n",
       "    (wembedding): Embedding(83, 32)\n",
       "    (lstm): LSTM(32, 256, batch_first=True)\n",
       "  )\n",
       "  (rl): RelationalLayer(\n",
       "    (f_fc1): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (mha_fc1): SEAttend(\n",
       "      (excite): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "      (attend): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (identity_fc1): Identity()\n",
       "    (f_fc2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (mha_fc2): SEAttend(\n",
       "      (excite): Sequential(\n",
       "        (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "      (attend): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (identity_fc2): Identity()\n",
       "    (f_fc3): Linear(in_features=256, out_features=28, bias=True)\n",
       "    (mha_fc3): SEAttend(\n",
       "      (excite): Sequential(\n",
       "        (0): Linear(in_features=28, out_features=16, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=16, out_features=28, bias=True)\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "      (attend): MultiheadAttention(\n",
       "        (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        (norm): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "    )\n",
       "    (identity_fc3): Identity()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (g_layers): ModuleList(\n",
       "      (0): Linear(in_features=308, out_features=256, bias=True)\n",
       "      (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (mha_layers): ModuleList(\n",
       "      (0): SEAttend(\n",
       "        (excite): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "        (attend): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=308, out_features=308, bias=True)\n",
       "          (norm): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SEAttend(\n",
       "        (excite): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "        (attend): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (norm): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SEAttend(\n",
       "        (excite): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "        (attend): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (norm): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SEAttend(\n",
       "        (excite): Sequential(\n",
       "          (0): Linear(in_features=256, out_features=16, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): Linear(in_features=16, out_features=256, bias=True)\n",
       "          (3): Sigmoid()\n",
       "        )\n",
       "        (attend): MultiheadAttention(\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (norm): InstanceNorm1d(1, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (identity_layers): ModuleList(\n",
       "      (0): Identity()\n",
       "      (1): Identity()\n",
       "      (2): Identity()\n",
       "      (3): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supposing original DeepMind model\n",
      "wights torch.Size([256, 256])\n",
      "attn size torch.Size([8, 1, 32])\n",
      "attn size torch.Size([8, 4096, 32])\n",
      "x size torch.Size([8, 4096, 256])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (32) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-4db77f979887>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclevr_test_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-c407fceaee0a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, qst_idxs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mqst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqst_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-c407fceaee0a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, qst)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attn size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x size'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_layer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                 \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_layer_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mattn_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                 \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_layer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (256) must match the size of tensor b (32) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "model = RN(args, hyp)\n",
    "model = nn.DataParallel(model)\n",
    "# model.load_state_dict(torch.load('./best_weights/SE_norm_0.5_reg.pth', map_location='cpu'))\n",
    "model = model.module\n",
    "model;\n",
    "\n",
    "b = next(iter(clevr_test_loader))\n",
    "model(b['image'], b['question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'state_description': False,\n",
       " 'g_layers': [256, 256, 256, 256],\n",
       " 'question_injection_position': 0,\n",
       " 'f_fc1': 256,\n",
       " 'f_fc2': 256,\n",
       " 'dropout': 0.5,\n",
       " 'lstm_hidden': 256,\n",
       " 'lstm_word_emb': 32,\n",
       " 'rl_in_size': 52}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'masks {attn_output_weights}')\n",
    "print(f'masks shape {attn_output_weights.shape}')\n",
    "_m = torch.zeros_like(attn_output_weights)\n",
    "print(f'zeros shape {_m.shape}')\n",
    "top_index = (-attn_output_weights).argsort(dim=2)[:, :, :128]\n",
    "print(f'top index shape {top_index.shape}')\n",
    "print(f'top index {top_index}')\n",
    "_m[top_index] = attn_output_weights[top_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = torch.randn(2, 1, 256)\n",
    "top_index = (-masks).argsort(dim=2)[:, :, :128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 256]), torch.Size([2, 1, 128]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks.size(), top_index.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 110 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-e47efb8531ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 110 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "masks[top_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supposing original DeepMind model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f7017c44d94205a96730675e9dcdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=16), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.9943, grad_fn=<AddBackward0>)\n",
      "tensor(2.9943, grad_fn=<MeanBackward0>)\n",
      "item 2.994332790374756\n",
      "tensor(0.0235, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-193-b470f289f05d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoord_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0malgo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclevr_test_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-192-7877570c2219>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(data, model, epoch, dictionaries, args)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tensor_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml1_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-79c6cce1698b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, qst_idxs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mqst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqst_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-186-79c6cce1698b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, qst)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# Pass through multiheadattention layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1369\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = RN(args, hyp)\n",
    "model = nn.DataParallel(model)\n",
    "model.load_state_dict(torch.load('./best_weights/SE_norm_0.5_reg.pth', map_location='cpu'))\n",
    "model = model.module\n",
    "model;\n",
    "model.coord_tensor = None\n",
    "algo = test(clevr_test_loader, model, 0, dictionaries, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.22157167, 0.00833366, 0.03545763, 0.11640876, 0.6828096 ]],\n",
       "\n",
       "       [[0.77533526, 0.29543232, 0.31299817, 0.42692693, 0.84035245]]])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.random.uniform(0, 1, (2, 1, 5))\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = np.sort((mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.00833366, 0.03545763, 0.11640876, 0.22157167, 0.6828096 ]],\n",
       "\n",
       "       [[0.29543232, 0.31299817, 0.42692693, 0.77533526, 0.84035245]]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top[:,:,2:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.        , 0.        , 0.45450132, 0.        ]],\n",
       "\n",
       "       [[0.        , 0.        , 0.        , 0.        , 0.29772767]]])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top = (-mask).argsort()\n",
    "top = (top < 2) * 1\n",
    "mask * top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 1, 2), torch.Size([2, 1, 256]))"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1]],\n",
       "\n",
       "       [[0, 3]]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[0, 1]], [[0, 3]]]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-163-151313a4468f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-163-151313a4468f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    mask[*top.tolist()]\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mask[top.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-10724c6efacb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0m_m\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0m_m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "_m = np.zeros_like(mask)\n",
    "_m[top] = mask[top]\n",
    "_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85873339, 0.75114839])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[(-mask).argsort()[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.15547956, 0.22081248, 0.50479333, 0.75114839, 0.85873339])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask[mask.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot specify order when the array has no fields.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-6a59970dc0d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msort\u001b[0;34m(a, axis, kind, order)\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"K\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot specify order when the array has no fields."
     ]
    }
   ],
   "source": [
    "np.sort(mask, order=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 26, 191,  93, 247, 228,  85,  60,  11,  99, 241,  82, 224, 109, 239,\n",
       "         201, 119,  20,  19,  61, 194,   6, 161, 152,  44, 125, 219,  84, 123,\n",
       "          87,  28,  56, 114, 225, 146, 113,  38, 112,  33, 129, 179,  94, 182,\n",
       "         208, 212, 235,  92, 127, 217, 207,  83, 249,  58, 170,   1, 103, 231,\n",
       "         132,  13, 237, 227,  15,  72, 117, 160,  96, 234,  95, 177, 138,   0,\n",
       "          79, 140,  53,  63, 211, 139, 157, 104, 193,  69,  30, 130, 137, 159,\n",
       "         136, 196, 150,  70, 243, 190, 198,  27, 101,  75,  10,  35, 253,  46,\n",
       "          66, 163, 111, 248, 105, 154,  36,  98,  40,  51, 216,  41, 134,  43,\n",
       "         131, 121, 166, 115,   3,  47,  32, 209, 200, 255, 133, 151, 164, 128,\n",
       "         156, 148, 246,  18,  74,   8, 106,  17, 175, 203,  89,   9,   7, 202,\n",
       "         176, 206, 195, 199, 210, 141, 172, 197, 186, 110, 189,  14,  65,  86,\n",
       "         116, 102, 118, 226,  25,  88,  76, 145, 100,  78, 173, 180,  37, 230,\n",
       "         205,  29,  34, 143, 254, 245,  24,  22, 187,  16, 184, 229,  31,  67,\n",
       "         178, 155, 250, 144, 142, 220, 174, 108, 158, 149,   4,  42,   5, 233,\n",
       "          68,  54,  71,  62, 238,  80, 240, 171,  50,  97, 252,  23, 244, 221,\n",
       "          90, 165, 147, 168, 181, 223,  49, 222, 183,  39,  91,  12,  64,  81,\n",
       "         153, 162, 236, 218, 124, 251, 167, 122,  55, 192,  77, 188, 169, 232,\n",
       "         135,  48, 242, 204,  73, 215, 185, 120,  57,  45, 107, 126, 214,  59,\n",
       "          52,   2,  21, 213],\n",
       "        [ 94, 151, 116,  57, 143, 255,  22,  74, 118,  41, 149, 187, 107, 208,\n",
       "         222, 197, 249, 194,  32, 177, 202,  83, 232,  60, 250,  28,  79, 173,\n",
       "         141, 211, 235, 171, 119,  11, 251, 254,  13,  16,  33, 159, 203,  19,\n",
       "         233,  68, 225, 127, 163,  99,  15,  90, 157,  49,  35, 248,  86,  42,\n",
       "         140, 199, 221,  63,  55,  81, 162, 103,  89, 209,  62, 110,  40,   3,\n",
       "           2, 114, 144, 133,   5, 246, 178, 161, 191, 128, 184, 207, 185, 126,\n",
       "         238,  84, 158, 152, 183,  95,  23, 106, 205,  17,  46,  71,  54,  34,\n",
       "         146,  92, 216, 101, 150, 242, 182,  37, 172, 180, 124, 231, 155, 243,\n",
       "          88,  14,  82,  56, 239, 181, 167,  75, 227,  51,   0, 165, 213,  21,\n",
       "         156, 123, 113, 230, 125, 252,   9, 164, 134,  69, 196,  12, 139,  59,\n",
       "          30, 247,  24,  44,  77,  61, 237, 129,   6,  66, 111, 153, 121,  53,\n",
       "          50,  47,   7,  25, 253, 170, 132,  85, 223, 244, 186,  18,  76, 245,\n",
       "          38,  45, 148, 236, 102, 200,  70, 154,  97,  52, 176,  91,  36,  78,\n",
       "         214, 160, 136,  58,  87, 174,  20, 120, 109, 210, 188, 138,  96,  29,\n",
       "         147, 212, 217,  67,  48,  64,  72, 166, 215, 117, 195, 108, 130, 142,\n",
       "          10, 229,  26, 131, 175, 240, 198, 241, 112, 100,  39, 179,  73, 218,\n",
       "         190, 168, 224, 228,  65,  98, 201, 219, 169,  27,  43, 135, 105, 189,\n",
       "         193, 234, 137,  93, 192,  80, 226, 104, 122,  31,   4,   8, 145, 204,\n",
       "         115, 206,   1, 220]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(2, 256).argsort(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 3 4 2 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.50100204, 0.59806581, 0.64691903, 0.71714421, 0.80007776])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mask.argsort())\n",
    "mask[mask.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-974814a0ddab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "mask[(mask).argsort()[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.44464914, 0.84900637, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask * ((-mask).argsort() < 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
