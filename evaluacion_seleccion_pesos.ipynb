{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:35:25.522838Z",
     "start_time": "2019-08-30T18:35:25.399160Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:35:26.945976Z",
     "start_time": "2019-08-30T18:35:26.914475Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "from MultiheadAttention import MultiheadAttention\n",
    "\n",
    "\n",
    "MULTIHEADATTENTION_HEADS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:35:28.101879Z",
     "start_time": "2019-08-30T18:35:28.038439Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class SE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, r = 16):\n",
    "        super(SE, self).__init__()\n",
    "\n",
    "        self.r = r\n",
    "        self.scale = int(input_dim/self.r)\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, self.scale)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(self.scale, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.norm = nn.InstanceNorm1d(1, affine = False)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tgt_len, bsz, embed_dim = x.size()\n",
    "        out = self.relu(self.fc1(x.view(bsz, embed_dim)))\n",
    "        out = self.fc2(out)\n",
    "        out = self.norm(out.unsqueeze(1))\n",
    "        out = self.sigmoid(out)     \n",
    "\n",
    "        return out, out.view(bsz, 1, self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:35:29.651612Z",
     "start_time": "2019-08-30T18:35:29.613197Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def squeeze(weights):\n",
    "    return weights.mean(dim=1)\n",
    "\n",
    "class SEAttend(nn.Module):\n",
    "    def __init__(self, in_dim=256, out_dim=256, squeeze_dim=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = in_dim # Cantidad neuronas capa anterior\n",
    "        self.out_dim = out_dim # Cantidad de neuronas capa siguiente (mascara sobre estas)\n",
    "        \n",
    "        self.excite = nn.Sequential(\n",
    "            nn.Linear(out_dim, squeeze_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(squeeze_dim, out_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        self.attend = MultiheadAttention(\n",
    "            in_dim,\n",
    "            MULTIHEADATTENTION_HEADS,\n",
    "            dropout=0.1,\n",
    "        )\n",
    "    \n",
    "    def forward(self, qst, weights):\n",
    "        # qst: [target_length, bsz, emb_dim]\n",
    "        # weights: [sequence_length, emb_dim]\n",
    "        bsz = qst.size(1)\n",
    "                \n",
    "        scale = self.squeeze(weights) # scale: [out_dim (256)]\n",
    "        scale = self.excite(scale.unsqueeze(0)) # scale: [1, out_dim (256)]\n",
    "        weights = weights * scale.t() # weights: [out_dim, in_dim]\n",
    "        weights = weights.unsqueeze(1).expand(self.out_dim, bsz, self.in_dim) # weights: [out_dim, bsz, in_dim]\n",
    "        \n",
    "        _, attn_output_weights = self.attend(qst, weights, weights)\n",
    "                \n",
    "        # Retorno None para mantener el formato\n",
    "        return None, attn_output_weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def squeeze(weights):\n",
    "        return squeeze(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:56:38.747394Z",
     "start_time": "2019-08-30T18:56:38.714412Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0, dtype=torch.float).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:56:45.862000Z",
     "start_time": "2019-08-30T18:56:45.721291Z"
    },
    "code_folding": [
     0,
     30,
     51
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ConvInputModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvInputModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm1 = nn.BatchNorm2d(24)\n",
    "        self.conv2 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm2 = nn.BatchNorm2d(24)\n",
    "        self.conv3 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm3 = nn.BatchNorm2d(24)\n",
    "        self.conv4 = nn.Conv2d(24, 24, 3, stride=2, padding=1)\n",
    "        self.batchNorm4 = nn.BatchNorm2d(24)\n",
    "        \n",
    "    def forward(self, img):\n",
    "        \"\"\"convolution\"\"\"\n",
    "        x = self.conv1(img)\n",
    "        x = self.batchNorm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchNorm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchNorm3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchNorm4(x)\n",
    "        x = F.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class QuestionEmbedModel(nn.Module):\n",
    "    def __init__(self, in_size, embed=32, hidden=128):\n",
    "        super(QuestionEmbedModel, self).__init__()\n",
    "        \n",
    "        self.wembedding = nn.Embedding(in_size + 1, embed)  #word embeddings have size 32\n",
    "        self.lstm = nn.LSTM(embed, hidden, batch_first=True)  # Input dim is 32, output dim is the question embedding\n",
    "        self.hidden = hidden\n",
    "        \n",
    "    def forward(self, question):\n",
    "        #calculate question embeddings\n",
    "        wembed = self.wembedding(question)\n",
    "        # wembed = wembed.permute(1,0,2) # in lstm minibatches are in the 2-nd dimension\n",
    "        self.lstm.flatten_parameters()\n",
    "        _, hidden = self.lstm(wembed) # initial state is set to zeros by default\n",
    "        qst_emb = hidden[0] # hidden state of the lstm. qst = (B x 128)\n",
    "        #qst_emb = qst_emb.permute(1,0,2).contiguous()\n",
    "        #qst_emb = qst_emb.view(-1, self.hidden*2)\n",
    "        qst_emb = qst_emb[0]\n",
    "        \n",
    "        return qst_emb\n",
    "\n",
    "class RelationalLayerBase(nn.Module):\n",
    "    def __init__(self, in_size, out_size, qst_size, hyp):\n",
    "        super().__init__()\n",
    "\n",
    "        # f_fc1\n",
    "        self.f_fc1 = nn.Linear(hyp[\"g_layers\"][-1], hyp[\"f_fc1\"])\n",
    "        # self.mha_fc1 = SEAttend(hyp[\"g_layers\"][-1], hyp[\"f_fc1\"])\n",
    "        self.identity_fc1 = nn.Identity()\n",
    "        # f_fc2\n",
    "        self.f_fc2 = nn.Linear(hyp[\"f_fc1\"], hyp[\"f_fc2\"])\n",
    "        # self.mha_fc2 = SEAttend(hyp[\"f_fc1\"], hyp[\"f_fc2\"])\n",
    "        self.identity_fc2 = nn.Identity()\n",
    "        # f_fc3\n",
    "        self.f_fc3 = nn.Linear(hyp[\"f_fc2\"], out_size)\n",
    "        # self.mha_fc3 = SEAttend(hyp[\"f_fc2\"], out_size)\n",
    "        self.identity_fc3 = nn.Identity()\n",
    "    \n",
    "        self.dropout = nn.Dropout(p=hyp[\"dropout\"])\n",
    "        \n",
    "        self.on_gpu = False\n",
    "        self.hyp = hyp\n",
    "        self.qst_size = qst_size\n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "\n",
    "    def cuda(self, device=None):\n",
    "        self.on_gpu = True\n",
    "        super().cuda(device)\n",
    "    \n",
    "\n",
    "class RelationalLayer(RelationalLayerBase):\n",
    "    def __init__(self, in_size, out_size, qst_size, hyp, extraction=False):\n",
    "        super().__init__(in_size, out_size, qst_size, hyp)\n",
    "\n",
    "        self.quest_inject_position = hyp[\"question_injection_position\"]\n",
    "        self.in_size = in_size\n",
    "\n",
    "        self.identity_layers = []\n",
    "\n",
    "\t    #create all g layers\n",
    "        self.g_layers = []\n",
    "        self.g_layers_size = hyp[\"g_layers\"]\n",
    "        for idx,g_layer_size in enumerate(hyp[\"g_layers\"]):\n",
    "            in_s = in_size if idx==0 else hyp[\"g_layers\"][idx-1]\n",
    "            out_s = g_layer_size\n",
    "            if idx==self.quest_inject_position:\n",
    "                #create the h layer. Now, for better code organization, it is part of the g layers pool. \n",
    "                l = nn.Linear(in_s+qst_size, out_s)\n",
    "            else:\n",
    "                #create a standard g layer.\n",
    "                l = nn.Linear(in_s, out_s)\n",
    "            self.g_layers.append(l)\n",
    "            self.identity_layers.append(nn.Identity())\n",
    "            \n",
    "        self.g_layers = nn.ModuleList(self.g_layers)\n",
    "        self.identity_layers = nn.ModuleList(self.identity_layers)\n",
    "        self.extraction = extraction\n",
    "\n",
    "    \n",
    "    def forward(self, x, qst):\n",
    "        # x = (B x 8*8 x 24)\n",
    "        # qst = (B x 128)\n",
    "        \"\"\"g\"\"\"\n",
    "        b, d, k = x.size()\n",
    "        qst_size = qst.size()[1]\n",
    "        \n",
    "        # add question everywhere\n",
    "        qst = torch.unsqueeze(qst, 1)                      # (B x 1 x 128)\n",
    "        qst = qst.repeat(1, d, 1)                       # (B x 64 x 128)\n",
    "        qst = torch.unsqueeze(qst, 2)                      # (B x 64 x 1 x 128)\n",
    "        \n",
    "        # cast all pairs against each other\n",
    "        x_i = torch.unsqueeze(x, 1)                   # (B x 1 x 64 x 26)\n",
    "        x_i = x_i.repeat(1, d, 1, 1)                    # (B x 64 x 64 x 26)\n",
    "        x_j = torch.unsqueeze(x, 2)                   # (B x 64 x 1 x 26)\n",
    "        #x_j = torch.cat([x_j, qst], 3)\n",
    "        x_j = x_j.repeat(1, 1, d, 1)                    # (B x 64 x 64 x 26)\n",
    "        \n",
    "        # concatenate all together\n",
    "        x_full = torch.cat([x_i, x_j], 3)                  # (B x 64 x 64 x 2*26)\n",
    "        \n",
    "        # reshape for passing through network\n",
    "        x_ = x_full.view(b * d**2, self.in_size)\n",
    "\n",
    "        #create g and inject the question at the position pointed by quest_inject_position.\n",
    "        for idx, (g_layer, g_layer_size, identity) in enumerate(zip(self.g_layers, self.g_layers_size, self.identity_layers)):\n",
    "            if idx==self.quest_inject_position:\n",
    "                in_size = self.in_size if idx==0 else self.g_layers_size[idx-1]\n",
    "\n",
    "                # questions inserted\n",
    "                x_img = x_.view(b,d,d,in_size)\n",
    "                qst = qst.repeat(1,1,d,1)\n",
    "                x_concat = torch.cat([x_img,qst],3) #(B x 64 x 64 x 128+256)\n",
    "\n",
    "                # h layer\n",
    "                x_ = x_concat.view(b*(d**2),in_size+self.qst_size)\n",
    "                x_ = g_layer(x_)\n",
    "                x_ = F.relu(x_)\n",
    "            else:\n",
    "                x_ = g_layer(x_)\n",
    "                x_ = F.relu(x_)\n",
    "            x_ = identity(x_)\n",
    "\n",
    "        if self.extraction:\n",
    "            return None\n",
    "        \n",
    "        # reshape again and sum\n",
    "        x_g = x_.view(b, d**2, self.g_layers_size[-1])\n",
    "        x_g = x_g.sum(1).squeeze(1)\n",
    "        \n",
    "        \"\"\"f\"\"\"\n",
    "        x_f = self.f_fc1(x_g)\n",
    "        x_f = F.relu(x_f)\n",
    "        x_f = self.identity_fc1(x_f)\n",
    "\n",
    "        x_f = self.f_fc2(x_f)\n",
    "        x_f = self.dropout(x_f)\n",
    "        x_f = F.relu(x_f)\n",
    "        x_f = self.identity_fc2(x_f)\n",
    "\n",
    "        x_f = self.f_fc3(x_f)\n",
    "        x_f = self.identity_fc3(x_f)\n",
    "\n",
    "        return F.log_softmax(x_f, dim=1), torch.tensor(0, dtype=torch.float)\n",
    "    \n",
    "\n",
    "class RN(nn.Module):\n",
    "    def __init__(self, args, hyp, extraction=False):\n",
    "        super(RN, self).__init__()\n",
    "        self.coord_tensor = None\n",
    "        self.on_gpu = False\n",
    "        self.use_images = args.use_images\n",
    "        \n",
    "        # CNN\n",
    "        self.conv = ConvInputModel()\n",
    "        self.state_desc = hyp['state_description']            \n",
    "            \n",
    "        # LSTM\n",
    "        hidden_size = hyp[\"lstm_hidden\"]\n",
    "        self.text = QuestionEmbedModel(args.qdict_size, embed=hyp[\"lstm_word_emb\"], hidden=hidden_size)\n",
    "        \n",
    "        # RELATIONAL LAYER\n",
    "        self.rl_in_size = hyp[\"rl_in_size\"]\n",
    "        self.rl_out_size = args.adict_size\n",
    "        self.rl = RelationalLayer(self.rl_in_size, self.rl_out_size, hidden_size, hyp, extraction) \n",
    "        if hyp[\"question_injection_position\"] != 0:          \n",
    "            print('Supposing IR model')\n",
    "        else:     \n",
    "            print('Supposing original DeepMind model')\n",
    "\n",
    "    def forward(self, img, qst_idxs):\n",
    "        if self.state_desc or self.use_images is False:\n",
    "            x = img # (B x 12 x 8) or (B x 64 x 26)\n",
    "        else:\n",
    "            x = self.conv(img)  # (B x 24 x 8 x 8)\n",
    "            b, k, d, _ = x.size()\n",
    "            x = x.view(b,k,d*d) # (B x 24 x 8*8)\n",
    "            \n",
    "            # add coordinates\n",
    "            if self.coord_tensor is None or torch.cuda.device_count() == 1:\n",
    "                self.build_coord_tensor(b, d)                  # (B x 2 x 8 x 8)\n",
    "                self.coord_tensor = self.coord_tensor.view(b,2,d*d) # (B x 2 x 8*8)\n",
    "            \n",
    "            x = torch.cat([x, self.coord_tensor], 1)    # (B x 24+2 x 8*8)\n",
    "            x = x.permute(0, 2, 1)    # (B x 64 x 24+2)\n",
    "        \n",
    "        qst = self.text(qst_idxs)\n",
    "        y = self.rl(x, qst)\n",
    "        return y\n",
    "       \n",
    "    # prepare coord tensor\n",
    "    def build_coord_tensor(self, b, d):\n",
    "        coords = torch.linspace(-d/2., d/2., d)\n",
    "        x = coords.unsqueeze(0).repeat(d, 1)\n",
    "        y = coords.unsqueeze(1).repeat(1, d)\n",
    "        ct = torch.stack((x,y))\n",
    "        # broadcast to all batches\n",
    "        # TODO: upgrade pytorch and use broadcasting\n",
    "        ct = ct.unsqueeze(0).repeat(b, 1, 1, 1)\n",
    "        self.coord_tensor = Variable(ct, requires_grad=False)\n",
    "        if self.on_gpu:\n",
    "            self.coord_tensor = self.coord_tensor.cuda()\n",
    "    \n",
    "    def cuda(self, device=None):\n",
    "        self.on_gpu = True\n",
    "        self.rl.cuda(device)\n",
    "        super(RN, self).cuda(device)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T17:44:30.845707Z",
     "start_time": "2019-08-30T17:44:30.774067Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def test(data, model, epoch, dictionaries, args):\n",
    "    model.eval()\n",
    "\n",
    "    # accuracy for every class\n",
    "    class_corrects = {}\n",
    "    # for every class, among all the wrong answers, how much are non pertinent\n",
    "    class_invalids = {}\n",
    "    # total number of samples for every class\n",
    "    class_n_samples = {}\n",
    "    # initialization\n",
    "    for c in dictionaries[2].values():\n",
    "        class_corrects[c] = 0.0\n",
    "        class_invalids[c] = 0.0\n",
    "        class_n_samples[c] = 0.0\n",
    "\n",
    "    corrects = 0.0\n",
    "    invalids = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    inverted_answ_dict = {v: k for k,v in dictionaries[1].items()}\n",
    "    sorted_classes = sorted(dictionaries[2].items(), key=lambda x: hash(x[1]) if x[1]!='number' else int(inverted_answ_dict[x[0]]))\n",
    "    sorted_classes = [c[0]-1 for c in sorted_classes]\n",
    "\n",
    "    confusion_matrix_target = []\n",
    "    confusion_matrix_pred = []\n",
    "\n",
    "    sorted_labels = sorted(dictionaries[1].items(), key=lambda x: x[1])\n",
    "    sorted_labels = [c[0] for c in sorted_labels]\n",
    "    sorted_labels = [sorted_labels[c] for c in sorted_classes]\n",
    "\n",
    "    avg_loss = 0.0\n",
    "    progress_bar = tqdm(data)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for batch_idx, sample_batched in enumerate(progress_bar):\n",
    "            img, qst, label = utils.load_tensor_data(sample_batched, args.cuda, args.invert_questions, volatile=True)\n",
    "\n",
    "            output, l1_reg = model(img, qst)\n",
    "            pred = output.data.max(1)[1]\n",
    "\n",
    "            loss = F.nll_loss(output, label) + args.l1_lambd * l1_reg.mean()\n",
    "\n",
    "            # compute per-class accuracy\n",
    "            pred_class = [dictionaries[2][o.item()+1] for o in pred]\n",
    "            real_class = [dictionaries[2][o.item()+1] for o in label.data]\n",
    "            for idx,rc in enumerate(real_class):\n",
    "                class_corrects[rc] += (pred[idx] == label.data[idx]).item()\n",
    "                class_n_samples[rc] += 1\n",
    "\n",
    "            for pc, rc in zip(pred_class,real_class):\n",
    "                class_invalids[rc] += (pc != rc)\n",
    "\n",
    "            for p,l in zip(pred, label.data):\n",
    "                confusion_matrix_target.append(sorted_classes.index(l))\n",
    "                confusion_matrix_pred.append(sorted_classes.index(p))\n",
    "\n",
    "            # compute global accuracy\n",
    "            corrects += (pred == label.data).sum().item()\n",
    "            assert corrects == sum(class_corrects.values()), 'Number of correct answers assertion error!'\n",
    "            invalids = sum(class_invalids.values())\n",
    "            n_samples += len(label)\n",
    "            assert n_samples == sum(class_n_samples.values()), 'Number of total answers assertion error!'\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                accuracy = corrects / n_samples\n",
    "                invalids_perc = invalids / n_samples\n",
    "                progress_bar.set_postfix(dict(acc='{:.2%}'.format(accuracy), inv='{:.2%}'.format(invalids_perc)))\n",
    "    \n",
    "    avg_loss /= len(data)\n",
    "    invalids_perc = invalids / n_samples      \n",
    "    global_accuracy = corrects / n_samples\n",
    "\n",
    "    print('Test Epoch {}: Accuracy = {:.2%} ({:g}/{}); Invalids = {:.2%} ({:g}/{}); Test loss = {}'.format(epoch, accuracy, corrects, n_samples, invalids_perc, invalids, n_samples, avg_loss))\n",
    "    for v in class_n_samples.keys():\n",
    "        accuracy = 0\n",
    "        invalid = 0\n",
    "        if class_n_samples[v] != 0:\n",
    "            accuracy = class_corrects[v] / class_n_samples[v]\n",
    "            invalid = class_invalids[v] / class_n_samples[v]\n",
    "        print('{} -- acc: {:.2%} ({}/{}); invalid: {:.2%} ({}/{})'.format(v,accuracy,class_corrects[v],class_n_samples[v],invalid,class_invalids[v],class_n_samples[v]))\n",
    "\n",
    "    dump_object = {\n",
    "        'class_corrects':class_corrects,\n",
    "        'class_invalids':class_invalids,\n",
    "        'class_total_samples':class_n_samples,\n",
    "        'confusion_matrix_target':confusion_matrix_target,\n",
    "        'confusion_matrix_pred':confusion_matrix_pred,\n",
    "        'confusion_matrix_labels':sorted_labels,\n",
    "        'global_accuracy':global_accuracy,\n",
    "        'global_invalids':invalids_perc\n",
    "    }\n",
    "    torch.cuda.empty_cache()\n",
    "    return avg_loss, dump_object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T17:44:27.509349Z",
     "start_time": "2019-08-30T17:44:27.481991Z"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "from clevr_dataset_connector import ClevrDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T19:02:42.555373Z",
     "start_time": "2019-08-30T19:02:42.506699Z"
    },
    "code_folding": [
     1
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 699960/699960 [00:13<00:00, 52152.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of word dict 82\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "hyp =  {\n",
    "    \"state_description\": False,\n",
    "    \"g_layers\": [256,256,256,256],\n",
    "    \"question_injection_position\": 0,\n",
    "\n",
    "    \"f_fc1\": 256,\n",
    "    \"f_fc2\": 256,\n",
    "\n",
    "    \"dropout\": 0.5,\n",
    "    \"lstm_hidden\": 256,\n",
    "    \"lstm_word_emb\": 32,\n",
    "    \"rl_in_size\": 52\n",
    "}\n",
    "\n",
    "args = edict(\n",
    "    batch_size=220,\n",
    "    bs_gamma=1,\n",
    "    bs_max=-1,\n",
    "    bs_step=20,\n",
    "    # clevr_dir='/Users/sebamenabar/Documents/datasets/CLEVR/CLEVR_v1.0',\n",
    "    clevr_dir='/storage1/datasets/CLEVR_CoGenT_v1.0/',\n",
    "    clip_norm=50,\n",
    "    comet=1,\n",
    "    config='config.json',\n",
    "    conv_transfer_learn=None,\n",
    "    dropout=-1, \n",
    "    epochs=400,\n",
    "    experiment='Norm',\n",
    "    freeze_RN=False,\n",
    "    invert_questions=True,\n",
    "    l1_lambd=0.0,\n",
    "    log_interval=10,\n",
    "    lr=5e-06,\n",
    "    lr_gamma=2,\n",
    "    lr_max=0.0005,\n",
    "    lr_step=20,\n",
    "    model='original-fp',\n",
    "    no_cuda=False,\n",
    "    no_invert_questions=False,\n",
    "    question_injection=-1,\n",
    "    resume=None,\n",
    "    resume_comet='',\n",
    "    resume_optimizer=None,\n",
    "    seed=42,\n",
    "    subset=1.0,\n",
    "    test=False,\n",
    "    test_batch_size=100,\n",
    "    use_images=True\n",
    ")\n",
    "\n",
    "if args.dropout > 0:\n",
    "    hyp['dropout'] = args.dropout\n",
    "if args.question_injection >= 0:\n",
    "    hyp['question_injection_position'] = args.question_injection\n",
    "\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "dictionaries = utils.build_dictionaries(args.clevr_dir, 'cogentA')\n",
    "\n",
    "args.qdict_size = len(dictionaries[0])\n",
    "args.adict_size = len(dictionaries[1])\n",
    "\n",
    "args.use_images = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:43:55.612417Z",
     "start_time": "2019-08-30T18:43:55.584996Z"
    }
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:47:49.574680Z",
     "start_time": "2019-08-30T18:44:48.851580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> using cached questions: questions/CLEVR_val_questions.pkl\n"
     ]
    }
   ],
   "source": [
    "args.use_images = True\n",
    "train_transforms = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                   transforms.Pad(8),\n",
    "                                   transforms.RandomCrop((128, 128)),\n",
    "                                   transforms.RandomRotation(2.8),  # .05 rad\n",
    "                                   transforms.ToTensor()])\n",
    "test_transforms = transforms.Compose([transforms.Resize((128, 128)),\n",
    "                                  transforms.ToTensor()])\n",
    "\n",
    "# clevr_dataset_train = ClevrDataset(args.clevr_dir, True, dictionaries, train_transforms, use_images=False)\n",
    "clevr_dataset_test = ClevrDataset(args.clevr_dir, False, dictionaries, test_transforms, use_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:48:03.602231Z",
     "start_time": "2019-08-30T18:48:03.553514Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T19:00:57.421923Z",
     "start_time": "2019-08-30T19:00:57.379695Z"
    }
   },
   "outputs": [],
   "source": [
    "use_subset = False\n",
    "workers = 8\n",
    "args.bs = 64\n",
    "subset_size = 256\n",
    "\n",
    "clevr_subset_test = Subset(clevr_dataset_test, np.arange(subset_size))\n",
    "\n",
    "if use_subset:\n",
    "    clevr_test_loader = DataLoader(clevr_subset_test, batch_size=args.bs,\n",
    "                                   shuffle=False, num_workers=workers, collate_fn=utils.collate_samples_from_pixels)  \n",
    "# clevr_train_loader = DataLoader(clevr_dataset_train, batch_size=args.bs,\n",
    "#                                 shuffle=False, num_workers=4, collate_fn=utils.collate_samples_from_pixels)\n",
    "else:\n",
    "    clevr_test_loader = DataLoader(clevr_dataset_test, batch_size=args.bs,\n",
    "                                   shuffle=False, num_workers=workers, collate_fn=utils.collate_samples_from_pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T18:43:41.604150Z",
     "start_time": "2019-08-30T18:43:41.575394Z"
    }
   },
   "outputs": [],
   "source": [
    "from model import RN\n",
    "from model_vanilla import RN as VanillaRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T19:00:06.343076Z",
     "start_time": "2019-08-30T19:00:06.227704Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supposing original DeepMind model\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['module.rl.mha_fc1.in_proj_weight', 'module.rl.mha_fc1.in_proj_bias', 'module.rl.mha_fc1.out_proj.weight', 'module.rl.mha_fc1.out_proj.bias', 'module.rl.mha_fc2.in_proj_weight', 'module.rl.mha_fc2.in_proj_bias', 'module.rl.mha_fc2.out_proj.weight', 'module.rl.mha_fc2.out_proj.bias', 'module.rl.mha_fc3.in_proj_weight', 'module.rl.mha_fc3.in_proj_bias', 'module.rl.mha_fc3.out_proj.weight', 'module.rl.mha_fc3.out_proj.bias'])\n"
     ]
    }
   ],
   "source": [
    "model = VanillaRN(args, hyp)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model = nn.DataParallel(model)\n",
    "print(model.load_state_dict(torch.load('/home/fcorencoret/RelationNetworks-CLEVR/Baseline_bug/best_weights_epoch_399.pth'), strict=False))\n",
    "model = model.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.clevr_dir = '/storage1/datasets/CLEVR_CoGenT_v1.0/'\n",
    "args.clevr_dir = '/storage1/datasets/CLEVR_v1.0/'\n",
    "dataset = 'clevr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> using cached questions: questions/CLEVR_test_questions.pkl\n"
     ]
    }
   ],
   "source": [
    "# args.clevr_dir = '/storage1/datasets/CLEVR_CoGenT_v1.0/'\n",
    "args.clevr_dir = '/storage1/datasets/CLEVR_v1.0/'\n",
    "dataset = 'clevr'\n",
    "# dataset = 'cogentA'\n",
    "\n",
    "clevr_dataset_train = ClevrDataset(\n",
    "    args.clevr_dir, False, dictionaries, train_transforms, use_images=False, dataset=dataset, test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supposing original DeepMind model\n",
      "_IncompatibleKeys(missing_keys=[], unexpected_keys=['module.rl.mha_fc1.in_proj_weight', 'module.rl.mha_fc1.in_proj_bias', 'module.rl.mha_fc1.out_proj.weight', 'module.rl.mha_fc1.out_proj.bias', 'module.rl.mha_fc2.in_proj_weight', 'module.rl.mha_fc2.in_proj_bias', 'module.rl.mha_fc2.out_proj.weight', 'module.rl.mha_fc2.out_proj.bias', 'module.rl.mha_fc3.in_proj_weight', 'module.rl.mha_fc3.in_proj_bias', 'module.rl.mha_fc3.out_proj.weight', 'module.rl.mha_fc3.out_proj.bias'])\n"
     ]
    }
   ],
   "source": [
    "args.use_images = True\n",
    "\n",
    "clevr_dataset_train.use_images = args.use_images\n",
    "# clevr_dataset_test.use_images = args.use_images\n",
    "\n",
    "model = VanillaRN(args, hyp)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model = nn.DataParallel(model)\n",
    "print(model.load_state_dict(torch.load('/home/fcorencoret/RelationNetworks-CLEVR/Baseline_bug/best_weights_epoch_399.pth'), strict=False))\n",
    "model = model.module\n",
    "\n",
    "use_subset = False\n",
    "workers = 8\n",
    "args.bs = 256\n",
    "subset_size = 256\n",
    "\n",
    "# clevr_dataset_train = ClevrDataset(args.clevr_dir, True, dictionaries, train_transforms, use_images=args.use_images)\n",
    "# clevr_dataset_test = ClevrDataset(args.clevr_dir, False, dictionaries, test_transforms, use_images=args.use_images)\n",
    "\n",
    "clevr_train_loader = DataLoader(clevr_dataset_train, batch_size=args.bs,\n",
    "                                    shuffle=False, num_workers=workers, collate_fn=utils.collate_samples_from_pixels)\n",
    "# clevr_test_loader = DataLoader(clevr_dataset_test, batch_size=args.bs,\n",
    "#                                shuffle=False, num_workers=workers, collate_fn=utils.collate_samples_from_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-30T19:01:30.166423Z",
     "start_time": "2019-08-30T19:01:00.037681Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f801222a5640a4815809cb3dfff2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=586), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/samenabar/.venvs/cuda10/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/samenabar/.venvs/cuda10/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/samenabar/.venvs/cuda10/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/mnt-homes/kraken/samenabar/code/dynamic-rn/dynamic-rn/clevr_dataset_connector.py\", line 132, in __getitem__\n    answer = utils.to_dictionary_indexes(self.dictionaries[1], current_question['answer'])\nKeyError: 'answer'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-80d99ddcc3df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclevr_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionaries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-fc6b37c6a6b0>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(data, model, epoch, dictionaries, args)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tensor_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_batched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/cuda10/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/cuda10/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1030\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/cuda10/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/cuda10/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.venvs/cuda10/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    367\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/samenabar/.venvs/cuda10/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/samenabar/.venvs/cuda10/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/samenabar/.venvs/cuda10/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/mnt-homes/kraken/samenabar/code/dynamic-rn/dynamic-rn/clevr_dataset_connector.py\", line 132, in __getitem__\n    answer = utils.to_dictionary_indexes(self.dictionaries[1], current_question['answer'])\nKeyError: 'answer'\n"
     ]
    }
   ],
   "source": [
    "test(clevr_train_loader, model, 0, dictionaries, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "214.633px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
